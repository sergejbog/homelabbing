# For more information on this configuration, see the complete reference guide at
# https://grafana.com/docs/tempo/latest/configuration/

# Enables result streaming from Tempo (to Grafana) via HTTP.
stream_over_http_enabled: true

# Configure the server block.
server:
  # Listen for all incoming requests on port 3200.
  http_listen_port: 3200

# The distributor receives incoming trace span data for the system.
distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: "0.0.0.0:4317" # This example repository only utilises the OTLP gRPC receiver on port 4317.

# The ingester receives data from the distributor and processes it into indices and blocks.
ingester:
  trace_idle_period: 10s # The length of time after a trace has not received spans to consider it complete and flush it.
  max_block_bytes: 1_000_000 # Cut the head block when it hits this size or
  max_block_duration: 5m # this much time passes

# The compactor block configures the compactor responsible for compacting TSDB blocks.
compactor:
  compaction:
    compaction_window: 1h # Blocks in this time window will be compacted together.
    max_block_bytes: 100_000_000 # Maximum size of a compacted block.
    block_retention: 1h # How long to keep blocks. Default is 14 days, this demo system is short-lived.
    compacted_block_retention: 10m # How long to keep compacted blocks stored elsewhere.

# Configuration block to determine where to store TSDB blocks.
storage:
  trace:
    backend: local # Use the local filesystem for block storage. Not recommended for production systems.
    block:
      bloom_filter_false_positive: .05 # Bloom filter false positive rate.  lower values create larger filters but fewer false positives.
    # Write Ahead Log (WAL) configuration.
    wal:
      path: /tmp/tempo/wal # Directory to store the the WAL locally.
    # Local configuration for filesystem storage.
    local:
      path: /tmp/tempo/blocks # Directory to store the TSDB blocks.
    # Pool used for finding trace IDs.
    pool:
      max_workers: 100 # Worker pool determines the number of parallel requests to the object store backend.
      queue_depth: 10000 # Maximum depth for the querier queue jobs. A job is required for each block searched.

# Configures the metrics generator component of Tempo.
metrics_generator:
  # This option only allows spans with end times that occur within the configured duration to be
  # considered in metrics generation. This filters out outdated spans.
  metrics_ingestion_time_range_slack: 300s # Increased to 5 minutes
  # Specifies which processors to use.
  processor:
    # Span metrics create metrics based on span type, duration, name and service.
    span_metrics:
      # Keep only essential dimensions to reduce cardinality
      dimensions:
        - http.method
        - http.status_code
        - service.version
        - deployment.environment.name
      # Configure histogram buckets for latency metrics
      histogram_buckets: [.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10]
    # Service graph metrics create node and edge metrics for determinng service interactions.
    service_graphs:
      max_items: 10000 # Reduced from 50000 to limit memory usage
      # Keep minimal dimensions for service graphs
      dimensions:
        - http.method
        - http.status_code
        - deployment.environment.name
    # Configure the local blocks processor.
    local_blocks:
      # Ensure that metrics blocks are flushed to storage so TraceQL metrics queries against historical data.
      flush_to_storage: true
      filter_server_spans: false # Don't filter server spans - we want all spans
  # The registry configuration determines how to process metrics.
  registry:
    collection_interval: 15s # Increased from 5s to allow more time for metrics accumulation
    # Configure extra labels to be added to metrics.
    external_labels:
      source: tempo # Add a `{source="tempo"}` label.
      cluster: "${CLUSTER_NAME}" # Add a `{cluster="..."}` label.
  # Configures where the store for metrics is located.
  storage:
    # WAL for metrics generation.
    path: /tmp/tempo/generator/wal
    # Where to remote write metrics to.
    remote_write:
      - url: http://mimir:9009/api/v1/push # URL of locally running Mimir instance.
        send_exemplars: true # Send exemplars along with their metrics.
  traces_storage:
    path: /tmp/tempo/generator/traces

# Global override configuration.
overrides:
  metrics_generator_processors: [
      "service-graphs",
      "span-metrics",
      "local-blocks",
    ] # The types of metrics generation to enable for each tenant.
  max_bytes_per_trace: 50_000_000
  max_traces_per_user: 50000
